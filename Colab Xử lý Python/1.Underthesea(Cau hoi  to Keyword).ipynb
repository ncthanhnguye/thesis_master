{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from underthesea import word_tokenize\n",
        "import pandas as pd\n",
        "import string\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('vietnamese-stopwords.txt', 'r', encoding='utf-8') as file:\n",
        "    stopwords = [line.strip() for line in file]\n",
        "\n",
        "df = pd.read_excel(\"12k_data_with_keywords.xlsx\")\n",
        "\n",
        "for noi_dung in df[\"Nội dung câu hỏi\"]:\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(noi_dung)\n",
        "    # Remove stopwords\n",
        "    filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
        "    # Extract unique keywords (remove duplicates)\n",
        "    keywords = list(set(filtered_tokens))\n",
        "    # Print the extracted keywords\n",
        "    print(keywords)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load Vietnamese stopwords from a text file\n",
        "with open('vietnamese-stopwords.txt', 'r', encoding='utf-8') as file:\n",
        "    stopwords = [line.strip() for line in file]\n",
        "\n",
        "# Read the Excel file\n",
        "df = pd.read_excel(\"12k_data_with_keywords.xlsx\")\n",
        "\n",
        "# Function to preprocess and extract keywords\n",
        "def extract_keywords(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # Remove stopwords\n",
        "    filtered_tokens = [token.lower() for token in tokens if token.lower() not in stopwords]\n",
        "    \n",
        "    return filtered_tokens\n",
        "\n",
        "# Apply the extract_keywords function to each row in the \"Nội dung câu hỏi\" column\n",
        "df['Keywords'] = df['Nội dung câu hỏi'].apply(extract_keywords)\n",
        "\n",
        "# Save the DataFrame with extracted keywords to a new Excel file\n",
        "output_file = \"12k_data_with_keywords.xlsx\"\n",
        "df.to_excel(output_file, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['2-3-2023', 'nộp', 'hồ sơ', 'lãnh', 'bhxh', '1', '10-03-2023', 'kết quả', 'điện thoại', 'bảo hiểm', 'bảo hiểm xã hội', 'tỉnh nam định', 'bảo hiểm xã hội', 'huyện nghĩa hưng số', '02653', '2023', '03605', 'tb', 'bhxh huyện nghĩa hưng', 'thông báo', '1', 'chấp nhận', 'giải quyết', 'hồ sơ', 'giao dịch', 'điện tử', 'tổ chức', 'hồ sơ', '01630', '2023', '03605', 'hồi', '15', '15', 'phút', '02/03/2023', 'kết quả', '09', '03', '2023', '2', 'trường hợp', 'hồ sơ', 'quy định', 'hướng dẫn', 'bổ sung', 'hồ sơ', 'kèm', 'điện thoại']\n"
          ]
        }
      ],
      "source": [
        "# ---------------------------------------\n",
        "# Input 1 Text\n",
        "with open('vietnamese-stopwords.txt', 'r', encoding='utf-8') as file:\n",
        "    stopwords = [line.strip() for line in file]\n",
        "\n",
        "a = \"Ngày 2-3-2023 tôi có nộp hồ sơ lãnh BHXH 1 lần . Đến nay là ngày 10-03-2023 nhưng tôi vẫn không nhận được kết quả hay điện thoại từ bên bảo hiểm. BẢO HIỂM XÃ HỘI TỈNH NAM ĐỊNH BẢO HIỂM XÃ HỘI HUYỆN NGHĨA HƯNG Số: 02653/2023/03605/TB BHXH HUYỆN NGHĨA HƯNG Thông báo 1. Chấp nhận giải quyết hồ sơ giao dịch điện tử của đơn vị/tổ chức/cá nhân Số hồ sơ: 01630/2023/03605. Nhận hồi: 15 giờ 15 phút, ngày 02/03/2023. Trả kết quả ngày 09 tháng 03 năm 2023. 2. Các trường hợp hồ sơ chưa đúng, chưa đủ theo quy định và hướng dẫn bổ sung hồ sơ (kèm theo) Số điện thoại hỗ trợ:\"\n",
        "# Function to preprocess and extract keywords\n",
        "def extract_keywords(text):\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    \n",
        "    # Remove stopwords\n",
        "    filtered_tokens = [token.lower() for token in tokens if token.lower() not in stopwords]\n",
        "    \n",
        "    return filtered_tokens\n",
        "\n",
        "print(extract_keywords(a))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9FTymN0sU1MM",
        "By-O7zw4Z3BR",
        "3UVSpU9sa0lI",
        "K7lmP4NxlK5E"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
