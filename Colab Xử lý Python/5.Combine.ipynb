{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#vncoreNLP\n",
        "!pip install deplacy vncorenlp\n",
        "\n",
        "from underthesea import word_tokenize\n",
        "import pandas as pd\n",
        "import string\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hợp đồng', 'lao động', 'thỏa thuận', 'lao động', 'lao động', 'việc làm', 'công', 'tiền lương', 'lao động', 'quyền', 'nghĩa vụ', 'quan hệ', 'lao động']\n",
            "Word: Hợp_đồng, POS: N, Head: 3, Dep Label: sub\n",
            "Word: lao_động, POS: N, Head: 1, Dep Label: nmod\n",
            "Word: là, POS: V, Head: 0, Dep Label: root\n",
            "Word: sự, POS: N, Head: 3, Dep Label: vmod\n",
            "Word: thoả_thuận, POS: V, Head: 4, Dep Label: nmod\n",
            "Word: giữa, POS: N, Head: 5, Dep Label: loc\n",
            "Word: người, POS: N, Head: 6, Dep Label: nmod\n",
            "Word: lao_động, POS: V, Head: 7, Dep Label: nmod\n",
            "Word: và, POS: Cc, Head: 4, Dep Label: nmod\n",
            "Word: người, POS: N, Head: 4, Dep Label: nmod\n",
            "Word: sử_dụng, POS: V, Head: 10, Dep Label: nmod\n",
            "Word: lao_động, POS: N, Head: 11, Dep Label: dob\n",
            "Word: về, POS: E, Head: 4, Dep Label: nmod\n",
            "Word: việc_làm, POS: N, Head: 13, Dep Label: pob\n",
            "Word: có, POS: V, Head: 4, Dep Label: nmod\n",
            "Word: trả, POS: V, Head: 15, Dep Label: vmod\n",
            "Word: công, POS: N, Head: 16, Dep Label: dob\n",
            "Word: ,, POS: CH, Head: 17, Dep Label: punct\n",
            "Word: tiền_lương, POS: N, Head: 17, Dep Label: nmod\n",
            "Word: ,, POS: CH, Head: 17, Dep Label: punct\n",
            "Word: điều_kiện, POS: N, Head: 17, Dep Label: nmod\n",
            "Word: lao_động, POS: N, Head: 21, Dep Label: nmod\n",
            "Word: ,, POS: CH, Head: 17, Dep Label: punct\n",
            "Word: quyền, POS: N, Head: 17, Dep Label: nmod\n",
            "Word: và, POS: Cc, Head: 17, Dep Label: coord\n",
            "Word: nghĩa_vụ, POS: N, Head: 25, Dep Label: conj\n",
            "Word: của, POS: E, Head: 17, Dep Label: nmod\n",
            "Word: mỗi, POS: L, Head: 29, Dep Label: det\n",
            "Word: bên, POS: N, Head: 27, Dep Label: pob\n",
            "Word: trong, POS: N, Head: 29, Dep Label: nmod\n",
            "Word: quan_hệ, POS: N, Head: 29, Dep Label: nmod\n",
            "Word: lao_động, POS: N, Head: 31, Dep Label: nmod\n",
            "Word: ., POS: CH, Head: 3, Dep Label: punct\n"
          ]
        }
      ],
      "source": [
        "from vncorenlp import VnCoreNLP\n",
        "\n",
        "vnp=VnCoreNLP(\"VnCoreNLP/VnCoreNLP-1.1.1.jar\",annotators=\"wseg,pos,parse\")\n",
        "nlp=lambda t:\"\".join([\"\\n\".join([\"\\t\".join([str(v[\"index\"]),v[\"form\"],\"_\",v[\"posTag\"],v[\"posTag\"],\"_\",str(v[\"head\"]),v[\"depLabel\"],\"_\",\"_\"]) for v in s])+\"\\n\\n\" for s in vnp.annotate(t)[\"sentences\"]])\n",
        "\n",
        "with open('vietnamese-stopwords.txt', 'r', encoding='utf-8') as file:\n",
        "    stopwords = [line.strip() for line in file]\n",
        "    \n",
        "noi_dung= 'Hợp đồng lao động là sự thỏa thuận giữa người lao động và người sử dụng lao động về việc làm có trả công, tiền lương, điều kiện lao động, quyền và nghĩa vụ của mỗi bên trong quan hệ lao động.'\n",
        "tokens = word_tokenize(noi_dung)\n",
        "# Remove stopwords\n",
        "filtered_tokens = [token for token in tokens if token.lower() not in stopwords]\n",
        "# Extract unique keywords (remove duplicates)\n",
        "keywords = list((filtered_tokens))\n",
        "# Print the extracted keywords\n",
        "print(keywords)\n",
        "\n",
        "\n",
        "annotated_text = vnp.annotate(noi_dung)\n",
        "# Iterate through the sentences and words and print them\n",
        "for sentence in annotated_text['sentences']:\n",
        "    for word in sentence:\n",
        "        print(f\"Word: {word['form']}, POS: {word['posTag']}, Head: {word['head']}, Dep Label: {word['depLabel']}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: spacy in c:\\python310\\lib\\site-packages (3.4.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in c:\\python310\\lib\\site-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\python310\\lib\\site-packages (from spacy) (1.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\python310\\lib\\site-packages (from spacy) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\python310\\lib\\site-packages (from spacy) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\python310\\lib\\site-packages (from spacy) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\python310\\lib\\site-packages (from spacy) (8.1.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\python310\\lib\\site-packages (from spacy) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\python310\\lib\\site-packages (from spacy) (2.4.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\python310\\lib\\site-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\python310\\lib\\site-packages (from spacy) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in c:\\python310\\lib\\site-packages (from spacy) (0.10.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\python310\\lib\\site-packages (from spacy) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\python310\\lib\\site-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\python310\\lib\\site-packages (from spacy) (2.26.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\python310\\lib\\site-packages (from spacy) (1.10.2)\n",
            "Requirement already satisfied: jinja2 in c:\\python310\\lib\\site-packages (from spacy) (3.1.2)\n",
            "Requirement already satisfied: setuptools in c:\\python310\\lib\\site-packages (from spacy) (57.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\python310\\lib\\site-packages (from spacy) (21.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\python310\\lib\\site-packages (from spacy) (3.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\python310\\lib\\site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\python310\\lib\\site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.1.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\dhkgn\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy) (0.4.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\python310\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python310\\lib\\site-packages (from jinja2->spacy) (2.1.1)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "     ---------------------------------------- 12.8/12.8 MB 3.1 MB/s eta 0:00:00\n",
            "Requirement already satisfied: spacy<3.5.0,>=3.4.0 in c:\\python310\\lib\\site-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in c:\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in c:\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in c:\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in c:\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in c:\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in c:\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in c:\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in c:\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in c:\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in c:\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: pathy>=0.3.5 in c:\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in c:\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.25.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.26.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: jinja2 in c:\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.1.2)\n",
            "Requirement already satisfied: setuptools in c:\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in c:\\python310\\lib\\site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\python310\\lib\\site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in c:\\python310\\lib\\site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in c:\\python310\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2021.10.8)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.9)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in c:\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in c:\\python310\\lib\\site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.1.3)\n",
            "Requirement already satisfied: colorama in c:\\users\\dhkgn\\appdata\\roaming\\python\\python310\\site-packages (from tqdm<5.0.0,>=4.38.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\python310\\lib\\site-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python310\\lib\\site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.1.1)\n",
            "✔ Download and installation successful\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 23.2.1 -> 23.3.2\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install spacy\n",
        "!python -m spacy download en_core_web_sm\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "import bs4\n",
        "import requests\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "from spacy.matcher import Matcher \n",
        "from spacy.tokens import Span \n",
        "\n",
        "import networkx as nx\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(13674, 7)"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# import wikipedia sentences\n",
        "candidate_sentences = pd.read_excel(\"12k_data_with_keywords.xlsx\")\n",
        "candidate_sentences.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_entities(sent):\n",
        "  ## chunk 1\n",
        "  ent1 = \"\"\n",
        "  ent2 = \"\"\n",
        "\n",
        "  prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
        "  prv_tok_text = \"\"   # previous token in the sentence\n",
        "\n",
        "  prefix = \"\"\n",
        "  modifier = \"\"\n",
        "\n",
        "  #############################################################\n",
        "  \n",
        "  for tok in nlp(sent):\n",
        "    ## chunk 2\n",
        "    # if token is a punctuation mark then move on to the next token\n",
        "    if tok.dep_ != \"punct\":\n",
        "      # check: token is a compound word or not\n",
        "      if tok.dep_ == \"compound\":\n",
        "        prefix = tok.text\n",
        "        # if the previous word was also a 'compound' then add the current word to it\n",
        "        if prv_tok_dep == \"compound\":\n",
        "          prefix = prv_tok_text + \" \"+ tok.text\n",
        "      \n",
        "      # check: token is a modifier or not\n",
        "      if tok.dep_.endswith(\"mod\") == True:\n",
        "        modifier = tok.text\n",
        "        # if the previous word was also a 'compound' then add the current word to it\n",
        "        if prv_tok_dep == \"compound\":\n",
        "          modifier = prv_tok_text + \" \"+ tok.text\n",
        "      \n",
        "      ## chunk 3\n",
        "      if tok.dep_.find(\"subj\") == True:\n",
        "        ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
        "        prefix = \"\"\n",
        "        modifier = \"\"\n",
        "        prv_tok_dep = \"\"\n",
        "        prv_tok_text = \"\"      \n",
        "\n",
        "      ## chunk 4\n",
        "      if tok.dep_.find(\"obj\") == True:\n",
        "        ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
        "        \n",
        "      ## chunk 5  \n",
        "      # update variables\n",
        "      prv_tok_dep = tok.dep_\n",
        "      prv_tok_text = tok.text\n",
        "  #############################################################\n",
        "\n",
        "  return [ent1.strip(), ent2.strip()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 13674/13674 [07:52<00:00, 28.92it/s]\n"
          ]
        }
      ],
      "source": [
        "entity_pairs = []\n",
        "\n",
        "for i in tqdm(candidate_sentences[\"Câu trả lời\"]):\n",
        "  entity_pairs.append(get_entities(i))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_relation(sent):\n",
        "\n",
        "  doc = nlp(sent)\n",
        "\n",
        "  # Matcher class object \n",
        "  matcher = Matcher(nlp.vocab)\n",
        "\n",
        "  #define the pattern \n",
        "  pattern = [{'DEP':'ROOT'}, \n",
        "            {'DEP':'prep','OP':\"?\"},\n",
        "            {'DEP':'agent','OP':\"?\"},  \n",
        "            {'POS':'ADJ','OP':\"?\"}] \n",
        "\n",
        "  matcher.add(\"matching_1\", [pattern]) \n",
        "\n",
        "  matches = matcher(doc)\n",
        "  k = len(matches) - 1\n",
        "\n",
        "  span = doc[matches[k][1]:matches[k][2]] \n",
        "\n",
        "  return(span.text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 13674/13674 [07:24<00:00, 30.78it/s]\n"
          ]
        }
      ],
      "source": [
        "relations = [get_relation(i) for i in tqdm(candidate_sentences['Câu trả lời'])]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "giờ hành    983\n",
              "gọi         798\n",
              "định        751\n",
              "số          633\n",
              "dẫn         502\n",
              "tổng        495\n",
              "Đề nghị     327\n",
              "động        295\n",
              "hệ          245\n",
              "bảo         241\n",
              "trợ         232\n",
              "Nam số      201\n",
              "cung        186\n",
              "thì         162\n",
              "gia         153\n",
              "tại         150\n",
              "quyết       149\n",
              "Việt        136\n",
              "thể         128\n",
              "này         122\n",
              "hưởng       114\n",
              "cấp         112\n",
              "công ty     112\n",
              "đáp         111\n",
              "hoàn        110\n",
              "được        110\n",
              "biết        110\n",
              "BHXH        100\n",
              "tôi          98\n",
              "mức          93\n",
              "tiếp         93\n",
              "có           92\n",
              "do           82\n",
              "nhận hồ      82\n",
              "bạn          80\n",
              "đài          80\n",
              "trên         79\n",
              "sổ           78\n",
              "định về      73\n",
              "Bạn          72\n",
              "BHYT         70\n",
              "thoại        69\n",
              "tháng        68\n",
              "tâm          63\n",
              "vậy          61\n",
              "kiểm         60\n",
              "nhất         60\n",
              "với          60\n",
              "sung         59\n",
              "hiện         59\n",
              "dtype: int64"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pd.Series(relations).value_counts()[:50]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# extract subject\n",
        "source = [i[0] for i in entity_pairs]\n",
        "\n",
        "# extract object\n",
        "target = [i[1] for i in entity_pairs]\n",
        "\n",
        "kg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 864x864 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "G=nx.from_pandas_edgelist(kg_df[kg_df['edge']==\"composed by\"], \"source\", \"target\", \n",
        "                          edge_attr=True, create_using=nx.MultiDiGraph())\n",
        "\n",
        "plt.figure(figsize=(12,12))\n",
        "pos = nx.spring_layout(G, k = 0.5) # k regulates the distance between nodes\n",
        "nx.draw(G, with_labels=True, node_color='skyblue', node_size=1500, edge_cmap=plt.cm.Blues, pos = pos)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9FTymN0sU1MM",
        "By-O7zw4Z3BR",
        "3UVSpU9sa0lI",
        "K7lmP4NxlK5E"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
