{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A more modern approach (in 2021) is to use a Machine Learning NLP model. There are pre-trained models exactly for this task, many of them are derived from BERT, so you don't have to train your own model (you could if you wanted to). Here is a code example that uses the excellent Huggingface Transformers library with PyTorch. It's based on this example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Downloading (…)okenizer_config.json: 100%|██████████| 29.0/29.0 [00:00<?, ?B/s]\n",
            "c:\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:137: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\dhkgn\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n",
            "Downloading (…)lve/main/config.json: 100%|██████████| 433/433 [00:00<?, ?B/s] \n",
            "Downloading (…)solve/main/vocab.txt: 100%|██████████| 213k/213k [00:00<00:00, 3.05MB/s]\n",
            "Downloading (…)/main/tokenizer.json: 100%|██████████| 436k/436k [00:00<00:00, 1.78MB/s]\n",
            "Downloading model.safetensors: 100%|██████████| 433M/433M [01:12<00:00, 5.95MB/s] \n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "not paraphrase: 10%\n",
            "is paraphrase: 90%\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "import torch\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "not paraphrase: 61%\n",
            "is paraphrase: 39%\n"
          ]
        }
      ],
      "source": [
        "model_name = \"bert-base-cased-finetuned-mrpc\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "sequence_0 = \"Những đứa trẻ đang chơi bóng rổ tại công viên gần nhà họ vào buổi chiều mặt trời tỏa sáng.\"\n",
        "sequence_1 = \"Những con chim đang hót vang trên những cành cây rậm rạp ở một khu rừng tĩnh lặng.\"\n",
        "\n",
        "tokens = tokenizer.encode_plus(sequence_0, sequence_1, return_tensors=\"pt\")\n",
        "classification_logits = model(**tokens)[0]\n",
        "results = torch.softmax(classification_logits, dim=1).tolist()[0]\n",
        "\n",
        "classes = [\"not paraphrase\", \"is paraphrase\"]\n",
        "for i in range(len(classes)):\n",
        "    print(f\"{classes[i]}: {round(results[i] * 100)}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# List of keywords\n",
        "keywords = ['việc làm', 'Hợp đồng', 'quan hệ', 'thỏa thuận', 'nghĩa vụ', 'quyền', 'công', 'tiền lương', 'lao động']\n",
        "\n",
        "# Define sentences as a list of keywords\n",
        "sentences = [keywords]\n",
        "\n",
        "# Create and train a Word2Vec model\n",
        "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)\n",
        "\n",
        "# Find word relations\n",
        "similar_words = {}\n",
        "for keyword in keywords:\n",
        "    similar_words[keyword] = model.wv.most_similar(keyword)\n",
        "\n",
        "# Print word relations\n",
        "for keyword in keywords:\n",
        "    print(f\"Words related to '{keyword}':\")\n",
        "    for word, similarity in similar_words[keyword]:\n",
        "        print(f\"- {word} (Similarity: {similarity:.2f})\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "9FTymN0sU1MM",
        "By-O7zw4Z3BR",
        "3UVSpU9sa0lI",
        "K7lmP4NxlK5E"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
